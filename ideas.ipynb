{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9344ee0",
   "metadata": {},
   "source": [
    "# Noteboot for all of out amazing ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9468bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 'standard' neural network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19011d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pole_length(env, q_network):\n",
    "    \"\"\"\n",
    "    This function runs your trained network on a specific pole length\n",
    "    You are not allowed to change this function\n",
    "    \"\"\"\n",
    "\n",
    "    wind = 25\n",
    "    state = env.reset()[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = q_network(state).argmax().item()\n",
    "        next_state, reward, done, _, __ = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if total_reward >= 500 and total_reward <= 1000:\n",
    "            if total_reward % wind == 0:\n",
    "\n",
    "                env.unwrapped.force_mag = 75\n",
    "\n",
    "        if total_reward > 1000:\n",
    "            env.unwrapped.force_mag = 25 + (0.01 * total_reward)\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b907a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('observation', 'action', 'next_observation', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"save a transtion\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "        print(self.memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb1aaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(epsilon, policy_net, env, obs):\n",
    "    p = random.uniform(0,1)\n",
    "    if p < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(obs)\n",
    "            action = torch.argmax(q_values)\n",
    "    return action\n",
    "\n",
    "def error(current_q, td_target, error_type):\n",
    "    if error_type == \"mse\":\n",
    "        error = (current_q - td_target) ** 2\n",
    "    \n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cad37f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(replay_buffer, batch_size):\n",
    "    \"\"\"Return tensors for observations, actions, rewards, next_observations, dones\"\"\"\n",
    "    transitions = replay_buffer.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.observation])\n",
    "    next_obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.next_observation])\n",
    "    a = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1)\n",
    "    r = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1)\n",
    "    done = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1)\n",
    "    return obs, a, r, next_obs, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([Transition(observation=tensor([-0.0027,  0.0318,  0.0185,  0.0030]), action=1, next_observation=array([-0.00207714,  0.22666481,  0.01860359, -0.28375697], dtype=float32), reward=1.0, done=False)], maxlen=200)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m capacity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     91\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m---> 92\u001b[0m plot_avg_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mbaseline_DQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 32\u001b[0m, in \u001b[0;36mbaseline_DQN\u001b[0;34m(learning_rate, gamma, episodes, hidden_dim, target_update, epsilon, capacity, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m pick_action(epsilon, policy_net, env, observation)\n\u001b[0;32m---> 32\u001b[0m     next_observation, reward, terminated, truncated, __ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m())\n\u001b[1;32m     33\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mpush(observation, action, next_observation, reward, terminated)\n\u001b[1;32m     34\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "def baseline_DQN(learning_rate, gamma, episodes, hidden_dim, target_update, epsilon, capacity, batch_size):\n",
    "\n",
    "    # Initialize the policy network and optimizer\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation, _ = env.reset()\n",
    "    policy_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    plot_avg_rewards = []\n",
    "\n",
    "    replay_buffer = ReplayMemory(capacity=capacity)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        observation, _ = env.reset()\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        terminated = False\n",
    "        truncated = False \n",
    "        step_count = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        #first action\n",
    "        action = env.action_space.sample()\n",
    "        next_observation, reward, terminated, truncated, __ = env.step(action) \n",
    "        replay_buffer.push(observation, action, next_observation, reward, terminated)\n",
    "        observation = next_observation\n",
    "          \n",
    "        while not terminated:\n",
    "\n",
    "            action = pick_action(epsilon, policy_net, env, observation)\n",
    "            next_observation, reward, terminated, truncated, __ = env.step(action)\n",
    "            replay_buffer.push(observation, action, next_observation, reward, terminated)\n",
    "            total_reward += reward\n",
    "\n",
    "            #do random steps until buffer has at least reached the batch_size\n",
    "            if step_count < batch_size:\n",
    "                step_count += 1\n",
    "                continue \n",
    "\n",
    "            #obs, a , r, obs_next, done = replay_buffer.sample(batch_size)\n",
    "            obs, a , r, obs_next, done = sample_batch(replay_buffer, batch_size)\n",
    "\n",
    "            if step_count % target_update:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            step_count += 1\n",
    "        \n",
    "        # update rule\n",
    "        #sample a batch\n",
    "            next_q_values = target_net(obs_next)\n",
    "            next_q_max = next_q_values.max(dim=1)\n",
    "            td_target = r + gamma * (1-done) * next_q_max\n",
    "\n",
    "            current_q = policy_net(obs).gather(1, a)\n",
    "\n",
    "            loss = error(current_q, td_target, \"mse\")\n",
    "            \n",
    "\n",
    "            #update weights of the NN/policy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "    \n",
    "        # if episode % target_update == 0:\n",
    "        #     # update target network\n",
    "        #     target = network.clone()\n",
    "        if episode % 25 == 0:\n",
    "            # calculate the avg rewards of the last 25 steps here\n",
    "            average = total_reward / 25\n",
    "            plot_avg_rewards.append(average)\n",
    "            total_reward = 0\n",
    "\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return plot_avg_rewards\n",
    "\n",
    "\n",
    "# Hyperparameters, do not change\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "hidden_dim = 32\n",
    "target_update = 500\n",
    "epsilon = 0.6\n",
    "capacity = 200\n",
    "batch_size = 20\n",
    "plot_avg_rewards = baseline_DQN(learning_rate, gamma, episodes, hidden_dim, target_update, epsilon, capacity, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb750d4",
   "metadata": {},
   "source": [
    "he TOLD ME to add that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0721f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(learning_rate, gamma, episode, hidden_dim, result):\n",
    "\n",
    "    result = np.array(result)\n",
    "    \n",
    "    y = [25*i for i in range(len(result))]\n",
    "    y_axis = np.array(y)\n",
    "\n",
    "    #plot graph\n",
    "    plt.plot(y_axis, result)\n",
    "    plt.xlabel('Number of Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Average rewards, Learning Rate: {}, Gamma: {}, Episode: {}, Hidden_dim: {}'.format(learning_rate, gamma, episode, hidden_dim))\n",
    "    plt.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(learning_rate, gamma, episodes, hidden_dim, plot_avg_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
