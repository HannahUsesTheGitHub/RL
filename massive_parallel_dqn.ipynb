{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6a3e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque, namedtuple\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Manager, Lock, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8142c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('observation', 'action', 'next_observation', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemoryMP(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        manager = Manager()\n",
    "        self.memory = manager.list()\n",
    "        self.capacity = capacity\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"save a transtion\"\"\"\n",
    "        with self.lock:\n",
    "            if len(self.memory) >= self.capacity:\n",
    "                self.memory.pop(0)\n",
    "                self.memory.append(Transition(*args))\n",
    "        #print(self.memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        with self.lock:\n",
    "            return random.sample(list(self.memory), batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32341c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(epsilon, policy_net, env, obs):\n",
    "    p = random.uniform(0,1)\n",
    "    if p < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(obs)\n",
    "            action = torch.argmax(q_values).item()\n",
    "    return action\n",
    "\n",
    "def error(current_q, td_target, error_type):\n",
    "    if error_type == \"mse\":\n",
    "        #error = (current_q - td_target) ** 2\n",
    "        compute_loss = nn.MSELoss()\n",
    "        error = compute_loss(current_q, td_target)\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(replay_buffer, batch_size):\n",
    "    \"\"\"Return tensors for observations, actions, rewards, next_observations, dones\"\"\"\n",
    "    transitions = replay_buffer.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.observation])\n",
    "    next_obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.next_observation])\n",
    "    a = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1)\n",
    "    r = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1)\n",
    "    done = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return obs, a, r, next_obs, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_agent():\n",
    "    def __init__(self, id, replay_buffer, policy_net):\n",
    "        self.id = id\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.policy_net = policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MP_DQN(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size, n_agents):\n",
    "\n",
    "    # Initialize the policy network and optimizer\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation, _ = env.reset()\n",
    "    policy_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    plot_avg_rewards = []\n",
    "\n",
    "    replay_buffer = ReplayMemoryMP(capacity=capacity) #initialize buffer, its shared between all agents\n",
    "    step_count = 0 #global step count\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range(n_agents):\n",
    "        p = Process(target=worker_fn, args=(i, replay_buffer))\n",
    "        p.start()\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        observation, _ = env.reset() #observation for first action\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        terminated = False\n",
    "        truncated = False \n",
    "        #step_count = 0\n",
    "\n",
    "          \n",
    "        while not terminated and not truncated:\n",
    "\n",
    "            #perform an action, get env response\n",
    "            action = pick_action(epsilon, policy_net, env, observation)\n",
    "            next_observation, reward, terminated, truncated, __ = env.step(action)\n",
    "            next_observation = torch.tensor(next_observation, dtype=torch.float32) \n",
    "            total_reward += reward\n",
    "\n",
    "            #add new data to buffer\n",
    "            replay_buffer.push(observation, action, next_observation, reward, terminated) \n",
    "\n",
    "\n",
    "            #do random steps until buffer has at least reached the batch_size\n",
    "            if len(replay_buffer) < batch_size:\n",
    "                step_count += 1\n",
    "                continue \n",
    "\n",
    "            obs, a , r, obs_next, done = sample_batch(replay_buffer, batch_size)\n",
    "\n",
    "\n",
    "            #update target network\n",
    "            if step_count % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            step_count += 1\n",
    "        \n",
    "        \n",
    "            next_q_values = target_net(obs_next)\n",
    "            next_q_max = next_q_values.max(dim=1, keepdim=True)[0].detach()\n",
    "            td_target = r + gamma * (1-done) * next_q_max\n",
    "\n",
    "            current_q = policy_net(obs).gather(1, a)\n",
    "\n",
    "            loss = error(current_q, td_target, \"mse\")\n",
    "            \n",
    "            #to see of weights are updating later\n",
    "            #old_weights = [p.clone() for p in policy_net.parameters()]\n",
    "\n",
    "\n",
    "            #update weights of the NN/policy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #tests if td_target is detatched - SHOULD NOT PRINT ANYTHING if it's implemented correctly\n",
    "            for name, param in target_net.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"{name} has gradient!\")\n",
    "\n",
    "            #check if weights are updating\n",
    "            # weight_diffs = [ (p - old_p).abs().mean().item() for p, old_p in zip(policy_net.parameters(), old_weights) ]\n",
    "            # print(\"Average weight change per param:\", weight_diffs)\n",
    "\n",
    "\n",
    "            #update observation and decay epsilon for the next step\n",
    "            observation = next_observation\n",
    "            epsilon = max(0.01, 0.995 * epsilon) # decay epsilon\n",
    "\n",
    "\n",
    "        if episode % 25 == 0:\n",
    "            # calculate the avg rewards of the last 25 steps\n",
    "            average = total_reward / 25\n",
    "            plot_avg_rewards.append(average)\n",
    "            #print(f'epsilon={epsilon:.3f}, total_reward={total_reward}')\n",
    "            total_reward = 0\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode {episode}/{episodes}')\n",
    "\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return plot_avg_rewards, policy_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(learning_rate, gamma, episode, result):\n",
    "\n",
    "    result = np.array(result)\n",
    "    \n",
    "    y = [25*i for i in range(len(result))]\n",
    "    y_axis = np.array(y)\n",
    "\n",
    "    #plot graph\n",
    "    plt.plot(y_axis, result)\n",
    "    plt.xlabel('Number of Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Average rewards, Learning Rate: {}, Gamma: {}, Episode: {}'.format(learning_rate, gamma, episode))\n",
    "    plt.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "target_update = 70\n",
    "epsilon = 1\n",
    "capacity = 10000\n",
    "batch_size = 32\n",
    "plot_avg_rewards, baseline_network = baseline_DQN(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size)\n",
    "\n",
    "plot_results(learning_rate, gamma, episodes, plot_avg_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
