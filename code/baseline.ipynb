{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9344ee0",
   "metadata": {},
   "source": [
    "# Noteboot for all of out amazing ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9468bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a6dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 'standard' neural network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19011d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pole_length(env, q_network):\n",
    "    \"\"\"\n",
    "    This function runs your trained network on a specific pole length\n",
    "    You are not allowed to change this function\n",
    "    \"\"\"\n",
    "\n",
    "    wind = 25\n",
    "    state = env.reset()[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = q_network(state).argmax().item()\n",
    "        next_state, reward, done, _, __ = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if total_reward >= 500 and total_reward <= 1000:\n",
    "            if total_reward % wind == 0:\n",
    "\n",
    "                env.unwrapped.force_mag = 75\n",
    "\n",
    "        if total_reward > 1000:\n",
    "            env.unwrapped.force_mag = 25 + (0.01 * total_reward)\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b907a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', \n",
    "                        ('observation', 'action', 'next_observation', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"save a transtion\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "        #print(self.memory)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1aaefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(epsilon, policy_net, env, obs):\n",
    "    p = random.uniform(0,1)\n",
    "    if p < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(obs)\n",
    "            action = torch.argmax(q_values).item()\n",
    "    return action\n",
    "\n",
    "def error(current_q, td_target, error_type):\n",
    "    if error_type == \"mse\":\n",
    "        #error = (current_q - td_target) ** 2\n",
    "        compute_loss = nn.MSELoss()\n",
    "        error = compute_loss(current_q, td_target)\n",
    "    \n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad37f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(replay_buffer, batch_size):\n",
    "    \"\"\"Return tensors for observations, actions, rewards, next_observations, dones\"\"\"\n",
    "    transitions = replay_buffer.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.observation])\n",
    "    next_obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.next_observation])\n",
    "    a = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1)\n",
    "    r = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1)\n",
    "    done = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    return obs, a, r, next_obs, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75edfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_DQN(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size):\n",
    "\n",
    "    # Initialize the policy network and optimizer\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation, _ = env.reset()\n",
    "    policy_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net = QNetwork(state_dim=4, action_dim=2)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    plot_avg_rewards = []\n",
    "\n",
    "    replay_buffer = ReplayMemory(capacity=capacity) #initialize buffer\n",
    "    step_count = 0 #global step count\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        observation, _ = env.reset() #observation for first action\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        terminated = False\n",
    "        truncated = False \n",
    "        # inside training loop before each episode\n",
    "        length = float(np.random.uniform(0.4, 1.8))\n",
    "        env.unwrapped.length = length\n",
    "\n",
    "        #step_count = 0\n",
    "\n",
    "          \n",
    "        while not terminated and not truncated:\n",
    "\n",
    "            #perform an action, get env response\n",
    "            action = pick_action(epsilon, policy_net, env, observation)\n",
    "            next_observation, reward, terminated, truncated, __ = env.step(action)\n",
    "            next_observation = torch.tensor(next_observation, dtype=torch.float32) \n",
    "            total_reward += reward\n",
    "\n",
    "            #add new data to buffer\n",
    "            replay_buffer.push(observation, action, next_observation, reward, terminated) \n",
    "\n",
    "\n",
    "            #do random steps until buffer has at least reached the batch_size\n",
    "            if len(replay_buffer) < batch_size:\n",
    "                step_count += 1\n",
    "                continue \n",
    "\n",
    "            obs, a , r, obs_next, done = sample_batch(replay_buffer, batch_size)\n",
    "\n",
    "\n",
    "            #update target network\n",
    "            if step_count % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            step_count += 1\n",
    "        \n",
    "        \n",
    "            next_q_values = target_net(obs_next)\n",
    "            next_q_max = next_q_values.max(dim=1, keepdim=True)[0].detach()\n",
    "            td_target = r + gamma * (1-done) * next_q_max\n",
    "\n",
    "            current_q = policy_net(obs).gather(1, a)\n",
    "\n",
    "            loss = error(current_q, td_target, \"mse\")\n",
    "            \n",
    "            #to see of weights are updating later\n",
    "            #old_weights = [p.clone() for p in policy_net.parameters()]\n",
    "\n",
    "\n",
    "            #update weights of the NN/policy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #tests if td_target is detatched - SHOULD NOT PRINT ANYTHING if it's implemented correctly\n",
    "            for name, param in target_net.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(f\"{name} has gradient!\")\n",
    "\n",
    "            #check if weights are updating\n",
    "            # weight_diffs = [ (p - old_p).abs().mean().item() for p, old_p in zip(policy_net.parameters(), old_weights) ]\n",
    "            # print(\"Average weight change per param:\", weight_diffs)\n",
    "\n",
    "\n",
    "            #update observation and decay epsilon for the next step\n",
    "            observation = next_observation\n",
    "            epsilon = max(0.01, 0.995 * epsilon) # decay epsilon\n",
    "\n",
    "\n",
    "        if episode % 25 == 0:\n",
    "            # calculate the avg rewards of the last 25 steps\n",
    "            average = total_reward / 25\n",
    "            plot_avg_rewards.append(average)\n",
    "            #print(f'epsilon={epsilon:.3f}, total_reward={total_reward}')\n",
    "            total_reward = 0\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode {episode}/{episodes}')\n",
    "\n",
    "    \n",
    "    env.close()\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "    model_path = \"weights/baseline_model1.pth\"\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    \n",
    "    return plot_avg_rewards, policy_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0721f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(learning_rate, gamma, episode, result):\n",
    "\n",
    "    result = np.array(result)\n",
    "    \n",
    "    y = [25*i for i in range(len(result))]\n",
    "    y_axis = np.array(y)\n",
    "\n",
    "    #plot graph\n",
    "    plt.plot(y_axis, result)\n",
    "    plt.xlabel('Number of Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Average rewards, Learning Rate: {}, Gamma: {}, Episode: {}'.format(learning_rate, gamma, episode))\n",
    "    plt.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f950652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/13f8x97d4417md9411_dmc5m0000gp/T/ipykernel_70481/2806284355.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.observation])\n",
      "/var/folders/38/13f8x97d4417md9411_dmc5m0000gp/T/ipykernel_70481/2806284355.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_obs = torch.stack([torch.tensor(o, dtype=torch.float32) for o in batch.next_observation])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/2000\n",
      "Episode 200/2000\n",
      "Episode 300/2000\n",
      "Episode 400/2000\n",
      "Episode 500/2000\n",
      "Episode 600/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m capacity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m----> 8\u001b[0m plot_avg_rewards, baseline_network \u001b[38;5;241m=\u001b[39m \u001b[43mbaseline_DQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plot_results(learning_rate, gamma, episodes, plot_avg_rewards)\n",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m, in \u001b[0;36mbaseline_DQN\u001b[0;34m(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#to see of weights are updating later\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#old_weights = [p.clone() for p in policy_net.parameters()]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#update weights of the NN/policy\u001b[39;00m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#tests if td_target is detatched - SHOULD NOT PRINT ANYTHING if it's implemented correctly\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RL/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RL/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/RL/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "gamma = 0.99\n",
    "episodes = 2000\n",
    "target_update = 500\n",
    "epsilon = 1\n",
    "capacity = 50000\n",
    "batch_size = 64\n",
    "plot_avg_rewards, baseline_network = baseline_DQN(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size)\n",
    "\n",
    "plot_results(learning_rate, gamma, episodes, plot_avg_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
