{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e739bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246113d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(learning_rate, gamma, episode, result):\n",
    "\n",
    "    result = np.array(result)\n",
    "    \n",
    "    y = [25*i for i in range(len(result))]\n",
    "    y_axis = np.array(y)\n",
    "\n",
    "    #plot graph\n",
    "    plt.plot(y_axis, result)\n",
    "    plt.xlabel('Number of Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Average rewards, Learning Rate: {}, Gamma: {}, Episode: {}'.format(learning_rate, gamma, episode))\n",
    "    plt.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31956002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 'standard' neural network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PERWithUsagePenalty:\n",
    "    def __init__(self, capacity, alpha=0.8, beta_usage=0.05, epsilon = 0.05):\n",
    "        \"\"\"\n",
    "        capacity: max number of transitions\n",
    "        alpha: how strongly TD-error affects sampling probability (0 = uniform)\n",
    "        beta_usage: gentle penalty factor for how often a transition has been used\n",
    "        epsilon: amount of randomness to influence the priorities\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pos = 0\n",
    "        self.alpha = alpha\n",
    "        self.beta_usage = beta_usage\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, observation, action, next_observation, reward, done):\n",
    "        \"\"\"Add a new transition with max priority.\"\"\"\n",
    "        # Check highest priority in the buffer, if buffer is empty then it is 1.0\n",
    "        max_priority = max([t['priority'] for t in self.buffer], default=1.0)\n",
    "        # Initialise the transition, making priority the highest possible and setting uses to 0\n",
    "        transition = {\n",
    "            'observation': observation,\n",
    "            'action': action,\n",
    "            'next_observation': next_observation,\n",
    "            'reward': reward,\n",
    "            'done': done,\n",
    "            'priority': max_priority,\n",
    "            'uses': 0\n",
    "        }\n",
    "        # If buffer is not yet full, add the transition\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "        # If buffer full, replace oldest transition\n",
    "        else:\n",
    "            self.buffer[self.pos] = transition\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample transitions according to adjusted priority (TD-error and usage penalty).\"\"\"\n",
    "        # Get priorities based on TD error from buffer\n",
    "        priorities = np.array([t['priority'] for t in self.buffer], dtype=np.float32)\n",
    "        probs = priorities ** self.alpha\n",
    "        # Make sure the probabilities sum to 1\n",
    "        probs /= probs.sum()\n",
    "        # Length of buffer\n",
    "        N = len(self.buffer)\n",
    "        # Compute normalised sampling probabilities from priorities (with epsilon-mixing for exploration)\n",
    "        probs = (1 - self.epsilon) * probs + self.epsilon * (1 / N)\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "\n",
    "        # Convert to tensors\n",
    "        obs = torch.stack([t['observation'] for t in batch])\n",
    "        actions = torch.tensor([t['action'] for t in batch], dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor([t['reward'] for t in batch], dtype=torch.float32).unsqueeze(1)\n",
    "        next_obs = torch.stack([t['next_observation'] for t in batch])\n",
    "        dones = torch.tensor([t['done'] for t in batch], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        # Increment usage count (used for penalty in update)\n",
    "        for t in batch:\n",
    "            t['uses'] += 1\n",
    "\n",
    "        return batch, obs, actions, rewards, next_obs, dones\n",
    "\n",
    "    def update_td_errors(self, batch, td_errors):\n",
    "        \"\"\"Update priorities based on TD-error and usage penalty.\"\"\"\n",
    "        for transition, td_error in zip(batch, td_errors):\n",
    "            td_abs = abs(td_error.item())\n",
    "            usage_penalty = np.exp(-self.beta_usage * transition['uses'])\n",
    "            # Lower the priority based on the usage penalty\n",
    "            new_priority = td_abs * usage_penalty\n",
    "            transition['priority'] = max(new_priority, 1e-5)  # avoid zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action(epsilon, policy_net, env, obs):\n",
    "    p = random.uniform(0,1)\n",
    "    if p < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(obs)\n",
    "            action = torch.argmax(q_values).item()\n",
    "    return action\n",
    "\n",
    "def error(current_q, td_target, error_type):\n",
    "    if error_type == \"mse\":\n",
    "        compute_loss = nn.MSELoss()\n",
    "        error = compute_loss(current_q, td_target)\n",
    "    \n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02026a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_usage_DQN(\n",
    "    learning_rate=0.002,\n",
    "    gamma=0.99,\n",
    "    episodes=2000,\n",
    "    target_update=500,\n",
    "    epsilon=1.0,\n",
    "    capacity=50000,\n",
    "    batch_size=64,\n",
    "    alpha=0.6,\n",
    "    beta_usage=0.05,\n",
    "    epsilon_usage=0.00,\n",
    "):\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "    observation, _ = env.reset()\n",
    "    observation = torch.tensor(observation, dtype=torch.float32)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    policy_net = QNetwork(state_dim, action_dim)\n",
    "    target_net = QNetwork(state_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    plot_avg_rewards = []\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    # Initialise buffer\n",
    "    replay_buffer = PERWithUsagePenalty(capacity=capacity, alpha=alpha, beta_usage=beta_usage, epsilon=epsilon_usage)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        observation, _ = env.reset()\n",
    "        observation = torch.tensor(observation, dtype=torch.float32)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Random pole length per episode\n",
    "        env.unwrapped.length = float(np.random.uniform(0.4, 1.8))\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            # Pick action\n",
    "            action = pick_action(epsilon, policy_net, env, observation)\n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_observation = torch.tensor(next_observation, dtype=torch.float32)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Push to buffer\n",
    "            replay_buffer.push(observation, action, next_observation, reward, terminated)\n",
    "\n",
    "            # Only update if enough samples\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch, obs, a, r, obs_next, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Compute TD target\n",
    "                next_q_max = target_net(obs_next).max(dim=1, keepdim=True)[0].detach()\n",
    "                td_target = r + gamma * (1 - done) * next_q_max\n",
    "\n",
    "                # Compute current Q\n",
    "                current_q = policy_net(obs).gather(1, a)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = error(current_q, td_target, \"mse\")\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update learning progress in buffer\n",
    "                td_errors = (td_target - current_q).detach()\n",
    "                replay_buffer.update_td_errors(batch, td_errors)\n",
    "\n",
    "            # Update target network\n",
    "            if step_count % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            observation = next_observation\n",
    "            epsilon = max(0.01, 0.995 * epsilon)\n",
    "            step_count += 1\n",
    "\n",
    "        # Logging\n",
    "        if episode % 25 == 0:\n",
    "            avg_reward = total_reward / 25\n",
    "            plot_avg_rewards.append(avg_reward)\n",
    "            total_reward = 0\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}/{episodes}\")\n",
    "\n",
    "    env.close()\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "    torch.save(policy_net.state_dict(), \"weights/PER_usage_model1.pth\")\n",
    "\n",
    "    return plot_avg_rewards, policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49463991",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "gamma = 0.99\n",
    "episodes = 500\n",
    "target_update = 500\n",
    "epsilon = 1\n",
    "capacity = 50000\n",
    "batch_size = 64\n",
    "plot_avg_rewards, baseline_network = per_usage_DQN(learning_rate, gamma, episodes, target_update, epsilon, capacity, batch_size)\n",
    "\n",
    "plot_results(learning_rate, gamma, episodes, plot_avg_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f58298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Hyperparameter options\n",
    "# alpha_vals = [0.6, 0.8]\n",
    "# beta_vals = [0.05, 0.1]\n",
    "# epsilon_vals = [0.0, 0.05]\n",
    "\n",
    "# # Store results\n",
    "# all_results = []\n",
    "\n",
    "# # Number of runs per combination\n",
    "# num_runs = 1\n",
    "\n",
    "# for alpha, beta, eps_usage in itertools.product(alpha_vals, beta_vals, epsilon_vals):\n",
    "#     run_rewards = []\n",
    "#     print(f\"\\nTraining with alpha={alpha}, beta_usage={beta}, epsilon_usage={eps_usage}\")\n",
    "    \n",
    "#     for run in range(num_runs):\n",
    "#         print(f\"  Run {run + 1}/{num_runs}\")\n",
    "#         plot_avg_rewards, _ = per_usage_DQN(\n",
    "#             learning_rate=0.002,\n",
    "#             gamma=0.99,\n",
    "#             episodes=500,\n",
    "#             target_update=500,\n",
    "#             epsilon=1.0,\n",
    "#             capacity=50000,\n",
    "#             batch_size=64,\n",
    "#             alpha=alpha,\n",
    "#             beta_usage=beta,\n",
    "#             epsilon_usage=eps_usage\n",
    "#         )\n",
    "#         overall_avg = sum(plot_avg_rewards) / len(plot_avg_rewards)\n",
    "#         run_rewards.append(overall_avg)\n",
    "    \n",
    "#     # Compute mean and std over runs\n",
    "#     mean_reward = np.mean(run_rewards)\n",
    "#     std_reward = np.std(run_rewards)\n",
    "    \n",
    "#     all_results.append({\n",
    "#         \"alpha\": alpha,\n",
    "#         \"beta_usage\": beta,\n",
    "#         \"epsilon_usage\": eps_usage,\n",
    "#         \"mean_reward\": mean_reward,\n",
    "#         \"std_reward\": std_reward\n",
    "#     })\n",
    "#     print(all_results)\n",
    "\n",
    "# # Save results\n",
    "# df_results = pd.DataFrame(all_results)\n",
    "# df_results.to_excel(\"hyperparam_sweep_results.xlsx\", index=False)\n",
    "# print(\"\\nHyperparameter sweep complete! Results saved to 'hyperparam_sweep_results.xlsx'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
